/*
 * OpenAI API Reference Server
 *
 * This is an API reference server that mimics openai server.
 *
 * API version: 0.1.0
 * Generated by: OpenAPI Generator (https://openapi-generator.tech)
 */

package openapi

type ChatCompletionRequest struct {

	Messages []MessagesInner `json:"messages"`

	Model Model `json:"model,omitempty"`

	FrequencyPenalty FrequencyPenalty `json:"frequency_penalty,omitempty"`

	LogitBias LogitBias `json:"logit_bias,omitempty"`

	Logprobs Logprobs `json:"logprobs,omitempty"`

	TopLogprobs TopLogprobs `json:"top_logprobs,omitempty"`

	MaxTokens MaxTokens `json:"max_tokens,omitempty"`

	N N `json:"n,omitempty"`

	PresencePenalty PresencePenalty `json:"presence_penalty,omitempty"`

	ResponseFormat ChatCompletionRequestResponseFormat `json:"response_format,omitempty"`

	Seed Seed `json:"seed,omitempty"`

	Stop Stop `json:"stop,omitempty"`

	Stream Stream `json:"stream,omitempty"`

	StreamOptions ChatCompletionRequestStreamOptions `json:"stream_options,omitempty"`

	Temperature Temperature `json:"temperature,omitempty"`

	TopP TopP `json:"top_p,omitempty"`

	Tools Tools `json:"tools,omitempty"`

	ToolChoice ToolChoice `json:"tool_choice,omitempty"`

	User User `json:"user,omitempty"`

	BestOf BestOf `json:"best_of,omitempty"`

	UseBeamSearch bool `json:"use_beam_search,omitempty"`

	TopK int32 `json:"top_k,omitempty"`

	MinP float32 `json:"min_p,omitempty"`

	RepetitionPenalty float32 `json:"repetition_penalty,omitempty"`

	LengthPenalty float32 `json:"length_penalty,omitempty"`

	EarlyStopping bool `json:"early_stopping,omitempty"`

	StopTokenIds StopTokenIds `json:"stop_token_ids,omitempty"`

	IncludeStopStrInOutput bool `json:"include_stop_str_in_output,omitempty"`

	IgnoreEos bool `json:"ignore_eos,omitempty"`

	MinTokens int32 `json:"min_tokens,omitempty"`

	SkipSpecialTokens bool `json:"skip_special_tokens,omitempty"`

	SpacesBetweenSpecialTokens bool `json:"spaces_between_special_tokens,omitempty"`

	TruncatePromptTokens TruncatePromptTokens `json:"truncate_prompt_tokens,omitempty"`

	// If true, the new message will be prepended with the last message if they belong to the same role.
	Echo bool `json:"echo,omitempty"`

	// If true, the generation prompt will be added to the chat template. This is a parameter used by chat template in tokenizer config of the model.
	AddGenerationPrompt bool `json:"add_generation_prompt,omitempty"`

	// If true, special tokens (e.g. BOS) will be added to the prompt on top of what is added by the chat template. For most models, the chat template takes care of adding the special tokens so this should be set to false (as is the default).
	AddSpecialTokens bool `json:"add_special_tokens,omitempty"`

	Documents Documents `json:"documents,omitempty"`

	ChatTemplate ChatTemplate `json:"chat_template,omitempty"`

	ChatTemplateKwargs ChatTemplateKwargs `json:"chat_template_kwargs,omitempty"`

	GuidedJson GuidedJson `json:"guided_json,omitempty"`

	GuidedRegex GuidedRegex `json:"guided_regex,omitempty"`

	GuidedChoice GuidedChoice `json:"guided_choice,omitempty"`

	GuidedGrammar GuidedGrammar `json:"guided_grammar,omitempty"`

	GuidedDecodingBackend GuidedDecodingBackend `json:"guided_decoding_backend,omitempty"`

	GuidedWhitespacePattern GuidedWhitespacePattern `json:"guided_whitespace_pattern,omitempty"`
}
